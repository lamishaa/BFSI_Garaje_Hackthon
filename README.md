# BFSI_Garaje_Hackthon
Data Exploration & Analysis:

Jupyter is ideal for loading the dataset, exploring its features, and visualizing the data (e.g., distributions, correlations, etc.).

You can interactively clean and preprocess the data, see immediate outputs, and make changes on the fly.

Experimentation & Model Building:

Machine learning workflows often involve trial and error (trying different models, tuning parameters), which is easy to manage with the notebook’s cell-based structure.

It’s very simple to test various models (e.g., Logistic Regression, Random Forest) and evaluate their performance step-by-step.

Visualization:

Jupyter makes it easy to plot charts (e.g., feature importance, confusion matrix) right alongside your code. You can see the results of your data visualizations and model performance immediately.

Documentation:

You can combine code, visualizations, and markdown cells to explain each step in detail. This makes the process more transparent, and you can later use it as the core of your final report.

This is important when presenting the methodology and insights in your GitHub repo.

Interactive Model Testing:

Jupyter allows you to run the model, modify it, and instantly see the impact of any changes, which is ideal for an iterative project like machine learning.

When to Switch to VS Code (Optional):
Once you've completed the exploratory analysis and model training in Jupyter, you can use VS Code for the following:

Organizing your project structure, writing reusable functions, and modularizing the code.

Building the Streamlit UI (if you decide to deploy your model).

Version control with Git and GitHub integration.

Running or deploying the final production version of the project.
